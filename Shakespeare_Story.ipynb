{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL8JcyketFK4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sadia-sust/dataset-finetune-gpt2/raw/main/Shakespeare-Writings.zip\n",
        "!unzip Shakespeare-Writings.zip\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9GimCehVU-XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# I have added code for pre-processing files as I wanted\n",
        "\n",
        "files = ['Macbeth.txt', 'THE-TRAGEDY-OF-TITUS.txt', 'THE-LIFE-AND-DEATH-OF-KING-RICHARD-THE SECOND.txt', 'romeo-juliet.txt', 'A-MIDSUMMER-NIGHT’S-DREAM.txt',\n",
        "         'All-Well-That-Ends-Well.txt',\n",
        "         'The-Tragedy-of-Hamlet.txt', 'The-Tragedy-of-Julius-Caesar.txt', 'The-Tragedy-of-King-Lear.txt', 'The-Tragedy-of-King-Richard.txt',\n",
        "         'The-Tragedy-of-Romeo-and-Juliet.txt', 'Measure-for-Measure.txt', 'Much-Ado-about-Nothing.txt', 'OTHELLO-THE-MOOR-OF-VENICE.txt', 'THE-WINTER’S-TALE.txt',\n",
        "         'The-Comedy-of-Errors.txt', 'The-Merchant-of-Venice.txt', 'The-Taming-of-the-Shrew.txt', 'The-Tempest.txt', 'Twelfth-Night.txt', 'The-Sonnets.txt']\n",
        "\n",
        "processed_data = []\n",
        "word_count = 0\n",
        "for file in files:\n",
        "  #print('file name ', file)\n",
        "  file_path = \"/content/Shakespeare-Writings/\" + file\n",
        "  with open(file_path) as f:\n",
        "    data = f.read()\n",
        "  cleaned_data = \"\"\n",
        "  #print('First name: ' + data[:50])\n",
        "  tokens = nltk.sent_tokenize(data)\n",
        "  for t in tokens:\n",
        "    temp_data = t.strip()\n",
        "    temp_data2 = re.sub('\\s+',' ', temp_data)\n",
        "    cleaned_data += re.sub('\\n','', temp_data2)\n",
        "    cleaned_data += t\n",
        "  processed_data.append(cleaned_data)\n",
        "  single_file_wc = len(cleaned_data.split())\n",
        "  print(f'File name: {file}, word count: {single_file_wc}')\n",
        "  word_count += single_file_wc\n",
        "\n",
        "print(len(processed_data))\n",
        "print('Total word tokens: ' + str(word_count))\n",
        "print(\"reading done\")\n",
        "#print(data[:200])\n",
        "\n",
        "#modified build text files method\n",
        "def build_text_files(data_files, dest_path):\n",
        "  f = open(dest_path, 'w')\n",
        "  format_data = \"\"\n",
        "  for text in data_files:\n",
        "    format_data += text\n",
        "  print(\"Train dataset length: \"+str(len(format_data)))\n",
        "  f.write(format_data)\n",
        "  f.close()\n",
        "\n",
        "#print(len(processed_data))\n",
        "#as is in tutorial - below all lines\n",
        "train, test = train_test_split(processed_data, test_size=0.15)\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))\n"
      ],
      "metadata": {
        "id": "1uxvvH08uZ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc train_dataset.txt\n",
        "!wc test_dataset.txt\n"
      ],
      "metadata": {
        "id": "wCt3hdapgKYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") #changed tokenizer\n",
        "\n",
        "train_path = 'train_dataset.txt' # as is\n",
        "test_path = 'test_dataset.txt' # as is"
      ],
      "metadata": {
        "id": "B0G9vUSy036A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as is in tutorial\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ],
      "metadata": {
        "id": "4TNgqZcK1Eo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "E0eDmWTdxqfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "\n",
        "#modified and added parameters in Training Arguments such as save_strategy, evaluation_strategy, push_to_hub, steps value, logging_strategy\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-shakespeare\", #The output directory\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy = \"steps\",\n",
        "    push_to_hub=True,\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "    eval_steps = 250, # Number of update steps between two evaluations.\n",
        "    save_steps= 550, # after # steps model is saved\n",
        "    warmup_steps=350,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    logging_strategy=\"steps\"\n",
        "    )\n",
        "\n",
        "#as is\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "mUqcbNZ37GAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "swjDCer4AqWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "story = pipeline('text-generation',model='./gpt2-shakespeare', tokenizer='gpt2', max_length = 300)\n"
      ],
      "metadata": {
        "id": "CNdbSAA1Ff93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story(\"romeo and juliet \")"
      ],
      "metadata": {
        "id": "eMwDgJ9tF6Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story(\"how art thou\")"
      ],
      "metadata": {
        "id": "J559Sf5Im9-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story(\"a tragedy story \")"
      ],
      "metadata": {
        "id": "xZA-qyPfpcCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "wntL6sJ1_EAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}